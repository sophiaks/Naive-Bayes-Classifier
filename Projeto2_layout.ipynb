{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Projeto 2 - Ciência dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nome: Amanda Rosa do Carmo\n",
    "\n",
    "Nome: Beatriz Cabral\n",
    "\n",
    "Nome: Sophia Kerber Shigueoka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serão permitidos grupos de três pessoas, mas com uma rubrica mais exigente. Grupos deste tamanho precisarão fazer um questionário de avaliação de trabalho em equipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Classificador automático de sentimento\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introdução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O objetivo desse projeto é criar um classificador capaz de analisar como a audiência de “13 reasons why”, uma série de televisão polêmica, está reagindo. Para tal fim, será feita uma análise da probabilidade de um tweet ser relevante dada as palavras do seu conteúdo, utilizando o princípio do algoritimo de Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Desenvolvimento e metodologia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   A fim de simplificar a explicação do processo de desenvolvimento do projeto, ele será dividido em  X etapas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETAPA 1 - Preparando o ambiente no jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nessa etapa, serão baixadas e importadas todas as bibliotecas relevantes para o código, bem como implementadas todas as funções a serem utilizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "#Instalando o tweepy\n",
    "!pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Importando as bibliotecas\n",
    "import tweepy\n",
    "import math\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import json\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def cleanup(text):\n",
    "    \"\"\"\n",
    "        Função de limpeza muito simples que troca alguns sinais básicos por espaços\n",
    "    \"\"\"\n",
    "    punctuation = '[!\\-.:?;•,]' # Note que os sinais [] são delimitadores de um conjunto.\n",
    "    pattern = re.compile(punctuation)\n",
    "    text_subbed = re.sub(pattern, ' ', text)\n",
    "    return text_subbed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### ETAPA 2 - Autenticando no  Twitter\n",
    "\n",
    "* Conta: ***@KerberSophie***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui será feita a autenticação no twitter, a partir de um código obtido no próprio site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Dados de autenticação do twitter:\n",
    "\n",
    "#Coloque aqui o identificador da conta no twitter: @KerberSophie\n",
    "\n",
    "#leitura do arquivo no formato JSON\n",
    "with open('auth.pass') as fp:    \n",
    "    data = json.load(fp)\n",
    "\n",
    "#Configurando a biblioteca. Não modificar\n",
    "# auth = tweepy.OAuthHandler(data['consumer_key'], data['consumer_secret'])\n",
    "# auth.set_access_token(data['access_token'], data['access_token_secret'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETAPA 3 - Escolha de um produto e coleta das mensagens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiramente, iremos fazer a coleta dos dados, nesse caso de tweets, a partir da keyword “13 reasons why” escolhida. Feita a coleta, os dados foram salvos dentro de uma planilha no excel. Para importar as mensagens do Twitter, foi necessário o uso da biblioteca Tweepy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantidade mínima de mensagens capturadas:\n",
    "Escolhemos 1480 a partir de um método empírico. Quando testamos com 1000 não conseguimos tweets suficientes, e chegamos\n",
    "a conclusão de que o problema era proveniente do set(), que diminuía a quantidade de tweets baixados. Assim, decidiu-se\n",
    "aumentar a quantidade de tweets para 1500, já que com 1000 ficaram falatando 360 tweets. Com 1500, ficamos com 20 tweets a mais.\n",
    "como eram somente 20 tweets, imaginamos que poucos deles seriam retweets, então tiramos exatamente 20 tweets dos 1500.\n",
    "Com 1480 tweets, conseguimos 399 tweets de teste, ou seja, faltou somente 1. não achamos que 1 tweets afetará tanto a base\n",
    "de dados, então decidimos manter os 1480."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Produto escolhido:\n",
    "produto = '13 Reasons Why'\n",
    "\n",
    "n = 1480\n",
    "#Quantidade mínima de mensagens para a base de treinamento:\n",
    "t = 600\n",
    "\n",
    "#Filtro de língua, escolha uma na tabela ISO 639-1.\n",
    "lang = 'pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capturando os dados do twitter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 812\n"
     ]
    }
   ],
   "source": [
    "# #Cria um objeto para a captura\n",
    "# api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "\n",
    "# #Inicia a captura, para mais detalhes: ver a documentação do tweepy\n",
    "# i = 1\n",
    "# msgs = []\n",
    "# for msg in tweepy.Cursor(api.search, q=produto, lang=lang).items():    \n",
    "#     msgs.append(msg.text.lower())\n",
    "#     i += 1\n",
    "#     if i > n:\n",
    "#         break\n",
    "\n",
    "# #Embaralhando as mensagens para reduzir um possível viés\n",
    "# list = []\n",
    "# shuffle(msgs)\n",
    "\n",
    "# #Usando o set() para tirar os retweets\n",
    "# listaset = set(msgs)\n",
    "\n",
    "# #Adicionando o resultado do set() a uma lista para evitar o erro TypeError: 'set' object is not subscriptable\n",
    "# for value in listaset:\n",
    "#     list.append(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvando os dados em uma planilha Excel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# #Linhas abaixo foram comentadas uma vez que o conjunto de dados era o mesmo, mas o número de tweets foi alterado.\n",
    "\n",
    "# # #Verifica se o arquivo não existe para não substituir um conjunto pronto\n",
    "# # if not os.path.isfile('./{0}.xlsx'.format(produto)):\n",
    "    \n",
    "#     #Abre o arquivo para escrita\n",
    "# writer = pd.ExcelWriter('{0} novo.xlsx'.format(produto))\n",
    "\n",
    "#     #divide o conjunto de mensagens em duas planilhas\n",
    "# dft = pd.DataFrame({'Treinamento' : pd.Series(list[:t])})\n",
    "# dft.to_excel(excel_writer = writer, sheet_name = 'Treinamento', index = False)\n",
    "\n",
    "# dfc = pd.DataFrame({'Teste' : pd.Series(list[t:])})\n",
    "# dfc.to_excel(excel_writer = writer, sheet_name = 'Teste', index = False)\n",
    "\n",
    "#     #fecha o arquivo\n",
    "# writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETAPA 4 - Classificação manual dos tweets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previamente, foram estabelecidas 4 categorias para a classificação das mensagens, entre elas,\n",
    "\n",
    "* \t***P*** - Positivo – se a mensagem transmitida for positiva;\n",
    "*\t***N*** - Negativo – se a mensagem transmitida for negativa;\n",
    "*\t***O*** - Irrelevante – se a mensagem transmitida não for relevante para análise e/ou não se encaixar em nenhuma das outras categorias;\n",
    "*\t***R*** - Reação – se a mensagem transmitida for uma reação sobre algum episódio, temporada, ou da série em geral.\n",
    "\n",
    "Estabelecidas as categorias e selecionados os tweets, será criada uma base de treinamento, na qual as mensagens serão qualificadas manualmente no excel de acordo com as categoria mais propícias. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### ETAPA 5 - Montando o Classificador Naive-Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "O algoritmo de Naive Bayes é um classificador probabilístico baseado no teorema de Bayes, utilizado no processo de machine learning. O algoritmo supõe que uma característica independe da outra para acontecer, ou seja, mesmo na presença de uma característica particular em uma classe, isso não afeta na probabilidade de qualquer característica ocorrer. O teorema de bayes é escrito da seguinte forma:\n",
    "\n",
    "*** colocar a formula de bayes ***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse método será utilizado no projeto, uma vez que permite calcular a probabilidade de uma mensagem ser positiva, por exemplo, dada as palavras utilizadas, assumindo que as palavras em um tweet não tem nenhuma relação entre elas.\n",
    "\n",
    "A partir do nosso modelo, poderíamos reescrever o teorema de bayes da seguinte forma:\n",
    "\n",
    "*** colocar a formula de bayes nosso modelo ***\n",
    "\n",
    "A variável C é a classe variável que representa se um tweet será positivo, negativo, irrelevante ou uma reação, a partir das condições estabelecidas (probabilidade de ocorrência de uma palavra dada as condições). A variável P representa as palavras ocorridas nos tweets. P pode ser:\n",
    "\n",
    "*** colocar os possiveis valores de P ***\n",
    "\n",
    "Substituindo P por cada uma das possíveis palavras, temos:\n",
    "\n",
    "*** colocar a formula de bayes nosso modelo  com valores de P***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para implementar esse algoritmo, uma nova tabela deve ser criada com as palavras e suas respectivas frequências relativas em cada uma das categorias. Porém antes disso, deverá ser feita uma limpeza das mensagens, removendo pontuações e caracteres que não convém a análise.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import math\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import json\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def cleanup(text):\n",
    "    \"\"\"\n",
    "        Função de limpeza muito simples que troca alguns sinais básicos por espaços\n",
    "    \"\"\"\n",
    "    punctuation = '[!\\-.:?;•,]' # Note que os sinais [] são delimitadores de um conjunto.\n",
    "    pattern = re.compile(punctuation)\n",
    "    text_subbed = re.sub(pattern, ' ', text)\n",
    "    return text_subbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Treinamento</th>\n",
       "      <th>Categoria</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alguém aq ja assistiu a 3° temp de 13 reasons ...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(na 3° temporada de 13 reasons why) eu estou t...</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13 ep de 13 reasons why p tentar fazer vc sent...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nossa nem sabia que tinha lançado a 3° tempora...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>estava começando a ver  o primeiro episódio de...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Treinamento Categoria\n",
       "0  alguém aq ja assistiu a 3° temp de 13 reasons ...         O\n",
       "1  (na 3° temporada de 13 reasons why) eu estou t...         R\n",
       "2  13 ep de 13 reasons why p tentar fazer vc sent...         N\n",
       "3  nossa nem sabia que tinha lançado a 3° tempora...         O\n",
       "4  estava começando a ver  o primeiro episódio de...         N"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "excel = pd.read_excel('13 Reasons Why v2.xlsx', sheet_name='Treinamento')\n",
    "excel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Teste</th>\n",
       "      <th>Clas_manual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a coisa mais idiota de 13 reasons why foi colo...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>atribuí nota 8 ao episódio 3x1 - yeah. i'm the...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rt @citou13reasons: ''quando você julga uma pe...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>terminei 13 reasons why também, meu hobbie é c...</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rt @wivisampaio: saiu a nova temporada de elit...</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Teste Clas_manual\n",
       "0  a coisa mais idiota de 13 reasons why foi colo...           N\n",
       "1  atribuí nota 8 ao episódio 3x1 - yeah. i'm the...           O\n",
       "2  rt @citou13reasons: ''quando você julga uma pe...           O\n",
       "3  terminei 13 reasons why também, meu hobbie é c...           R\n",
       "4  rt @wivisampaio: saiu a nova temporada de elit...           O"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "excel_treino = pd.read_excel('13 Reasons Why v2.xlsx', sheet_name='Teste')\n",
    "excel_treino.rename(columns={'Unnamed: 1':'Clas_manual'}, inplace=True)\n",
    "excel_treino.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reasons           0.062772\n",
       "13                0.059285\n",
       "why               0.059285\n",
       "de                0.054054\n",
       "a                 0.033130\n",
       "temporada         0.025283\n",
       "que               0.017437\n",
       "o                 0.012206\n",
       "eu                0.011334\n",
       "é                 0.011334\n",
       "foi               0.010462\n",
       "e                 0.009590\n",
       "série             0.008718\n",
       "da                0.008718\n",
       "essa              0.007847\n",
       "uma               0.007847\n",
       "q                 0.007847\n",
       "tá                0.007847\n",
       "mas               0.007847\n",
       "boa               0.006975\n",
       "não               0.006975\n",
       "muito             0.006975\n",
       "terceira          0.006975\n",
       "#bancodeseries    0.006103\n",
       "ao                0.006103\n",
       "atribuí           0.006103\n",
       "episódio          0.006103\n",
       "nota              0.006103\n",
       "foda              0.006103\n",
       "rt                0.005231\n",
       "                    ...   \n",
       "namoral           0.000872\n",
       "riverdale         0.000872\n",
       "pais              0.000872\n",
       "@guimenekkkj      0.000872\n",
       "ótima             0.000872\n",
       "fico              0.000872\n",
       "critiquei         0.000872\n",
       "world             0.000872\n",
       "'13               0.000872\n",
       "3º                0.000872\n",
       "finalmente        0.000872\n",
       "entende           0.000872\n",
       "ai                0.000872\n",
       "are               0.000872\n",
       "chorar            0.000872\n",
       "genial            0.000872\n",
       "bem               0.000872\n",
       "fudeu             0.000872\n",
       "enrolada          0.000872\n",
       "devia             0.000872\n",
       "ate               0.000872\n",
       "gostosa           0.000872\n",
       "li                0.000872\n",
       "julguei           0.000872\n",
       "can               0.000872\n",
       "@mmazzoco1        0.000872\n",
       "you               0.000872\n",
       "primeiro          0.000872\n",
       "gostou            0.000872\n",
       "deus              0.000872\n",
       "Length: 423, dtype: float64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Função para limpar e criar tabela de frequências relativas\n",
    "\n",
    "def FreqRel(cat):\n",
    "    texto_completo = ' '.join(excel[excel.Categoria==cat].Treinamento)\n",
    "    semhttp = re.sub(r'http\\S+', '', texto_completo)\n",
    "    text_limpo = cleanup(semhttp)\n",
    "    rel = text_limpo.split()\n",
    "    freq_rel = pd.Series(rel).value_counts(True)\n",
    "    return freq_rel\n",
    "\n",
    "FreqRel('P')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estas funções dividem a função anterior\n",
    "def Limpa(texto):\n",
    "    semhttp = re.sub(r'http\\S+', '', texto)\n",
    "    text_limpo = cleanup(semhttp)\n",
    "    rel = text_limpo.split()\n",
    "    return(rel)\n",
    "\n",
    "# def FreqRel(Limpa):\n",
    "#     print(Limpa(texto_completo))\n",
    "#     freq_rel = pd.Series(Limpa).value_counts(True)\n",
    "#     return freq_rel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Descobrindo P(N), P(R), P(P) e P(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Todas as palavras somadas:\n",
      "10095\n"
     ]
    }
   ],
   "source": [
    "texto_1 = ' '.join(excel[excel.Categoria=='O'].Treinamento)\n",
    "texto_2 = ' '.join(excel[excel.Categoria=='R'].Treinamento)\n",
    "texto_3 = ' '.join(excel[excel.Categoria=='P'].Treinamento)\n",
    "texto_4 = ' '.join(excel[excel.Categoria=='N'].Treinamento)\n",
    "\n",
    "texto_1 = Limpa(texto_1)\n",
    "texto_2 = Limpa(texto_2)\n",
    "texto_3 = Limpa(texto_3)\n",
    "texto_4 = Limpa(texto_4)\n",
    "\n",
    "#juntando todos os textos\n",
    "texto_enorme = texto_1 + texto_2 + texto_3 + texto_4\n",
    "set(texto_enorme)\n",
    "dic_global = 0\n",
    "for palavra in texto_enorme:\n",
    "        dic_global+=1\n",
    "    \n",
    "print('Todas as palavras somadas:')\n",
    "print(dic_global)\n",
    "\n",
    "def prob_cat(cat):\n",
    "    palavras = 0\n",
    "    texto_completo = ' '.join(excel[excel.Categoria==cat].Treinamento)\n",
    "    texto_completo.replace('•', '')\n",
    "    semhttp = re.sub(r'http\\S+', '', texto_completo)\n",
    "    text_limpo = cleanup(semhttp)\n",
    "    rel = text_limpo.split()\n",
    "    set(rel)\n",
    "    # Para verificar se o código era coerente, imprimimos o total de palavras de cada categoria\n",
    "    for palavra in rel:\n",
    "        palavras += 1\n",
    "    prob_final = palavras/dic_global\n",
    "    return prob_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilidade da Categoria 'Neutro'\n",
      "0.5090638930163447\n",
      "\n",
      "Probabilidade da Categoria 'Negativo'\n",
      "0.2220901436354631\n",
      "\n",
      "Probabilidade da Categoria 'Positivo'\n",
      "0.11362060425953442\n",
      "\n",
      "Probabilidade da Categoria 'Reação'\n",
      "0.15522535908865776\n",
      "\n",
      "Probabilidade Total (se tudo der certo, deve ser 1)\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Probabilidade da Categoria 'Neutro'\")\n",
    "print(prob_cat('O'))\n",
    "print('')\n",
    "print(\"Probabilidade da Categoria 'Negativo'\")\n",
    "print(prob_cat('N'))\n",
    "print('')\n",
    "print(\"Probabilidade da Categoria 'Positivo'\")\n",
    "print(prob_cat('P'))\n",
    "print('')\n",
    "print(\"Probabilidade da Categoria 'Reação'\")\n",
    "print(prob_cat('R'))\n",
    "\n",
    "\n",
    "\n",
    "print('')\n",
    "print(\"Probabilidade Total (se tudo der certo, deve ser 1)\")\n",
    "print(prob_cat('O') + prob_cat('N') + prob_cat('P') + prob_cat('R'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reasons             0.052150\n",
       "13                  0.051372\n",
       "why                 0.049621\n",
       "de                  0.047285\n",
       "a                   0.029967\n",
       "e                   0.025491\n",
       "temporada           0.018486\n",
       "elite               0.015178\n",
       "que                 0.013621\n",
       "eu                  0.011870\n",
       "o                   0.009924\n",
       "não                 0.009730\n",
       "assistir            0.009535\n",
       "rt                  0.008173\n",
       "pra                 0.006616\n",
       "3                   0.006616\n",
       "sobre               0.006616\n",
       "2                   0.006227\n",
       "é                   0.006032\n",
       "da                  0.005838\n",
       "do                  0.005838\n",
       "1                   0.005643\n",
       "ver                 0.005643\n",
       "terceira            0.005449\n",
       "vou                 0.005254\n",
       "séries              0.005254\n",
       "q                   0.005059\n",
       "to                  0.004670\n",
       "com                 0.004476\n",
       "se                  0.004476\n",
       "                      ...   \n",
       "show                0.000195\n",
       "marina              0.000195\n",
       "revoltado           0.000195\n",
       "chiba               0.000195\n",
       "wtf                 0.000195\n",
       "@lucasbelis         0.000195\n",
       "c                   0.000195\n",
       "qualquer            0.000195\n",
       "inesperadas         0.000195\n",
       "foto                0.000195\n",
       "@rebeldevermelho    0.000195\n",
       "@alencaarx_         0.000195\n",
       "education           0.000195\n",
       "assassinato         0.000195\n",
       "tempora…            0.000195\n",
       "confirmar           0.000195\n",
       "precisa             0.000195\n",
       "abençoe             0.000195\n",
       "totalmente          0.000195\n",
       "gat                 0.000195\n",
       "comigo              0.000195\n",
       "2t                  0.000195\n",
       "cabelo              0.000195\n",
       "jeito               0.000195\n",
       "70's                0.000195\n",
       "aiiii🥵😻             0.000195\n",
       "espero              0.000195\n",
       "qtas                0.000195\n",
       "talvez              0.000195\n",
       "pe…                 0.000195\n",
       "Length: 1288, dtype: float64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    Fqr = FreqRel('R')\n",
    "    Fqn = FreqRel('N')\n",
    "    Fqp = FreqRel('P')\n",
    "    Fqo = FreqRel('O')\n",
    "except: \n",
    "    pass\n",
    "\n",
    "FreqRel('O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "Neutro_set = set(FreqRel(0).index)\n",
    "Reacao_set = set(FreqRel('R').index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Verificando a performance\n",
    "\n",
    "Agora você deve testar o seu classificador com a base de Testes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   P(N | palavras) = P(N) * P(cada_palavra | N)\n",
    "\n",
    "NÃO É PRECISO DIVIDIR PELA PROBABILIDADE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_R = 1\n",
    "prob_tweet = 1\n",
    "dicR = {}\n",
    "for tweet in excel_treino['Teste']:\n",
    "    tweet = cleanup(tweet)\n",
    "    pal_tweet = tweet.split()\n",
    "    for palavra_teste in pal_tweet:\n",
    "        for palavra, freq in Fqr.items():\n",
    "            if palavra_teste in Fqr.items():\n",
    "                probabilidade = freq\n",
    "            else:\n",
    "                probabilidade = 1\n",
    "            prob_tweet = prob_tweet * freq\n",
    "    prob_tweet = prob_tweet * prob_cat('R')\n",
    "    dicR[tweet] = prob_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_N = 1\n",
    "prob_tweet = 1\n",
    "dicN = {}\n",
    "for tweet in excel_treino['Teste']:\n",
    "    tweet = cleanup(tweet)\n",
    "    pal_tweet = tweet.split()\n",
    "    for palavra_teste in pal_tweet:\n",
    "        for palavra, freq in Fqn.items():\n",
    "            print(palavra)\n",
    "            if palavra_teste in Fqn.items():\n",
    "                probabilidade = freq\n",
    "            else:\n",
    "                probabilidade = 1\n",
    "            prob_tweet = prob_tweet * freq\n",
    "    prob_tweet = prob_tweet * prob_cat('N')\n",
    "    dicN[tweet] = prob_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_P = 1\n",
    "prob_tweet = 1\n",
    "dicP = {}\n",
    "for tweet in excel_treino['Teste']:\n",
    "    tweet = cleanup(tweet)\n",
    "    pal_tweet = tweet.split()\n",
    "    for palavra_teste in pal_tweet:\n",
    "        for palavra, freq in Fqp.items():\n",
    "            if palavra_teste in Fqp.items():\n",
    "                probabilidade = freq\n",
    "            else:\n",
    "                probabilidade = 1\n",
    "            prob_tweet = prob_tweet * freq\n",
    "    prob_tweet = prob_tweet * prob_cat('P')\n",
    "    dicP[tweet] = prob_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_O = 1\n",
    "prob_tweet = 1\n",
    "dicO = {}\n",
    "for tweet in excel_treino['Teste']:\n",
    "    tweet = cleanup(tweet)\n",
    "    pal_tweet = tweet.split()\n",
    "    for palavra_teste in pal_tweet:\n",
    "        for palavra, freq in Fqo.items():\n",
    "            if palavra_teste in Fqo.items():\n",
    "                probabilidade = freq\n",
    "            else:\n",
    "                probabilidade = 1\n",
    "            prob_tweet = prob_tweet * freq\n",
    "    prob_tweet = prob_tweet * prob_cat('O')\n",
    "    dicO[tweet] = math.log(prob_tweet,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dicN['a coisa mais idiota de 13 reasons why foi colocar o zac e a cheryl nas fitas  botaram eles num nível de fdp igual aos outros desnecessário'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dicO['a coisa mais idiota de 13 reasons why foi colocar o zac e a cheryl nas fitas  botaram eles num nível de fdp igual aos outros desnecessário'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dicR['a coisa mais idiota de 13 reasons why foi colocar o zac e a cheryl nas fitas  botaram eles num nível de fdp igual aos outros desnecessário'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dicP['a coisa mais idiota de 13 reasons why foi colocar o zac e a cheryl nas fitas  botaram eles num nível de fdp igual aos outros desnecessário'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_classificados = []\n",
    "\n",
    "for tweet in excel_treino['Teste']:\n",
    "    tweet = cleanup(tweet)\n",
    "    pal_tweet = tweet.split()\n",
    "    maior = dicN[tweet]\n",
    "    if maior < dicP[tweet]:\n",
    "        maior = dicP\n",
    "        tweets_classificados.append('P')\n",
    "    elif maior < dicO[tweet]:\n",
    "        maior = dicO\n",
    "        tweets_classificados.append('O') \n",
    "    elif maior < dicR[tweet]:\n",
    "        maior = dicR\n",
    "        tweets_classificados.append('R')\n",
    "    else:\n",
    "        tweets_classificados.append('N')\n",
    "\n",
    "excel_treino['previsto'] = tweets_classificados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Acurácia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = 0\n",
    "falso_P = 0\n",
    "O = 0\n",
    "falso_O = 0\n",
    "R = 0\n",
    "falso_R = 0\n",
    "N = 0\n",
    "falso_N = 0\n",
    "\n",
    "\n",
    "total = len(excel_treino[\"Teste\"])\n",
    "\n",
    "for [e,i] in zip(excel_treino[\"Teste\"], excel_treino[\"previsto\"]):\n",
    "    if e == 1:\n",
    "        if i == 1:\n",
    "            P += 1\n",
    "        else: \n",
    "            falso_P += 1\n",
    "    if e == 2:            \n",
    "        if i == 2:\n",
    "            N +=1\n",
    "        else:\n",
    "            falso_N +=1\n",
    "    if e == 2:            \n",
    "        if i == 2:\n",
    "            O +=1\n",
    "        else:\n",
    "            falso_O +=1\n",
    "    if e == 2:            \n",
    "        if i == 2:\n",
    "            R +=1\n",
    "        else:\n",
    "            falso_R +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Positivos Verdadeiros: \", round((P/total)*100,2), \"%\")\n",
    "print(\"Positivos Falsos: \", round((falso_P/total)*100, 2), \"%\")\n",
    "print(\"Negativos Verdadeiros: \", round((N/total)*100,2), \"%\")\n",
    "print(\"Negativos Falsos: \", round((falso_N/total)*100, 2), \"%\")\n",
    "print(\"Neutros Verdadeiros: \", round((O/total)*100,2), \"%\")\n",
    "print(\"Neutros Falsos: \", round((falso_O/total)*100, 2), \"%\")\n",
    "print(\"Reações Verdadeiros: \", round((R/total)*100,2), \"%\")\n",
    "print(\"Reações Falsos: \", round((falso_R/total)*100, 2), \"%\")\n",
    "print(\"_\"*35)\n",
    "\n",
    "print(\"Acertos: \", round(((P/total)+(N/total)+(O/total)+(R/total))*100,2), \"%\")\n",
    "print(\"Erros: \", round(((falso_P/total)+(falso_N/total)+(falso_R/total)+(falso_O/total))*100,2), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **DASHBOARD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(tweets_classificados))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from time import sleep\n",
    "# import pandas as pd\n",
    "\n",
    "# for e in range(3):\n",
    "#     !jupyter nbconvert --to notebook --execute Projeto2_layout.ipynb\n",
    "#     dashboard = pd.readExcel(\"13 Reasons Why novo.xlsx\")\n",
    "#     print(f\"dashoard: {dashoard}\")\n",
    "#     sleep(5)\n",
    "\n",
    "\n",
    "# excel_treino.to_excel('fill_auto1.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "while True:\n",
    "    #getting new tweets\n",
    "    #Cria um objeto para a captura\n",
    "    api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "\n",
    "    #Inicia a captura, para mais detalhes: ver a documentação do tweepy\n",
    "    i = 1\n",
    "    msgs = []\n",
    "    for msg in tweepy.Cursor(api.search, q=produto, lang=lang).items():    \n",
    "        msgs.append(msg.text.lower())\n",
    "        i += 1\n",
    "        if i > n:\n",
    "            break\n",
    "\n",
    "    #Embaralhando as mensagens para reduzir um possível viés\n",
    "    lista = []\n",
    "    shuffle(msgs)\n",
    "\n",
    "    #Usando o set() para tirar os retweets\n",
    "    listaset = set(msgs)\n",
    "\n",
    "    #Adicionando o resultado do set() a uma lista para evitar o erro TypeError: 'set' object is not subscriptable\n",
    "    for value in listaset:\n",
    "        lista.append(value)\n",
    "    \n",
    "    #criando dataframe com novos tweets\n",
    "    df_teste = pd.DataFrame({'Teste' : pd.Series(lista)})\n",
    "    \n",
    "    #classifie new tweets\n",
    "    tweets_classificados = []\n",
    "\n",
    "    for tweet in excel_treino['Teste']:\n",
    "        tweet = cleanup(tweet)\n",
    "        pal_tweet = tweet.split()\n",
    "        maior = dicN[tweet]\n",
    "        if maior < dicP[tweet]:\n",
    "            maior = dicP\n",
    "            tweets_classificados.append('P')\n",
    "        elif maior < dicO[tweet]:\n",
    "            maior = dicO\n",
    "            tweets_classificados.append('O') \n",
    "        elif maior < dicR[tweet]:\n",
    "            maior = dicR\n",
    "            tweets_classificados.append('R')\n",
    "        else:\n",
    "            tweets_classificados.append('N')\n",
    "            \n",
    "    df_teste['Categorias'] = tweets_classificados\n",
    "    \n",
    "    #mostrar na dashboard:\n",
    "    \n",
    "\n",
    "    sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Concluindo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aperfeiçoamento:\n",
    "\n",
    "Os trabalhos vão evoluir em conceito dependendo da quantidade de itens avançados:\n",
    "\n",
    "* Limpar: \\n, :, \", ', (, ), etc SEM remover emojis\n",
    "* Corrigir separação de espaços entre palavras e emojis ou emojis e emojis\n",
    "* Propor outras limpezas e transformações que não afetem a qualidade da informação ou classificação\n",
    "* Criar categorias intermediárias de relevância baseadas na probabilidade: ex.: muito relevante, relevante, neutro, irrelevante, muito irrelevante (3 categorias: C, mais categorias conta para B)\n",
    "* Explicar por que não posso usar o próprio classificador para gerar mais amostras de treinamento\n",
    "* Propor diferentes cenários para Naïve Bayes fora do contexto do projeto\n",
    "* Sugerir e explicar melhorias reais com indicações concretas de como implementar (indicar como fazer e indicar material de pesquisa)\n",
    "* Montar um dashboard que periodicamente realiza análise de sentimento e visualiza estes dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Por que não se pode utilizar o próprio classificador para gerar mais amostras de treinamento?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Ao se utilizar o próprio classificador para gerar mais amostras de treinamento, o classificador acaba ficando \n",
    "    'viciado', ou seja, acaba-se por prejudicar os resultados obtidos, e eles acabam não tendo uma qualidade boa.\n",
    "    Seria o contrário de puxar mais tweets, que melhora a qualidade dos dados e dos resultados, por ter um espaço \n",
    "    amostral muito maior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outras limpezas e tranformações:\n",
    "___\n",
    "    - Trocar vírguças por espaços\n",
    "    - Remover URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outros cenários de Naive Bayes fora do contexto do projeto\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - Previsão de Chuva\n",
    "    - Filtros de e-mail (Spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referências"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Naive Bayes and Text Classification](https://arxiv.org/pdf/1410.5329.pdf)  **Mais completo**\n",
    "\n",
    "[A practical explanation of a Naive Bayes Classifier](https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/) **Mais simples**\n",
    "\n",
    "https://towardsdatascience.com/naive-bayes-classifier-81d512f50a7c\n",
    "\n",
    "https://www.geeksforgeeks.org/naive-bayes-classifiers/\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probP = 0\n",
    "# probN = 0\n",
    "# probR = 0\n",
    "# probO = 0\n",
    "# i = 0\n",
    "# dic_catR = {}\n",
    "# dic_catP = {}\n",
    "# dic_catN = {}\n",
    "# dic_catO = {}\n",
    "\n",
    "\n",
    "# #Para cada tweet na planilha:\n",
    "# for tweet in excel_treino['Teste']:\n",
    "#     tweet = cleanup(tweet.lower())\n",
    "#     pal_tweet = tweet.split()\n",
    "#     #Para cada palavra de cada tweet:\n",
    "#     for palavra in pal_tweet:\n",
    "#         um = True\n",
    "#         for palavra1, frequencia in Fqr.items():\n",
    "#             if um == True:\n",
    "#                 #Seta a frquencia inicial de cada palavra para 1\n",
    "#                 dic_catR[tweet] = 1\n",
    "#             um = False\n",
    "#             # probabilidade total são todas as probabilidades das palavras multiplicadas pela probabilidade de certa categoria\n",
    "#             dic_catR[tweet] = dic_catR[tweet] *  prob_cat('R') * frequencia\n",
    "                              \n",
    "#         for palavra1, frequencia in Fqn.items():\n",
    "#             if um == True:\n",
    "#                 dic_catN[tweet] = 1\n",
    "#             um = False\n",
    "#             dic_catN[tweet] = dic_catN[tweet] *  prob_cat('N') * frequencia\n",
    "            \n",
    "#         for palavra1, frequencia in Fqp.items():\n",
    "#             if um == True:\n",
    "#                 dic_catP[tweet] = 1\n",
    "#             um = False\n",
    "#             dic_catP[tweet] = dic_catP[tweet] *  prob_cat('P') * frequencia\n",
    "\n",
    "#         for palavra1, frequencia in Fqo.items():\n",
    "#             if um == True:\n",
    "#                 dic_catP[tweet] = 1\n",
    "#             um = False\n",
    "#             dic_catO[tweet] = dic_catP[tweet] *  prob_cat('O') * frequencia     \n",
    "\n",
    "#         #Não precisa dividir pela probabilidade do tweet pois cancelaria.\n",
    "    \n",
    "# # fazer contador pra ver qual é a maior probabilidade, e adicionar em uma nova coluna do dataframe\n",
    "# #  DataFrame.insert(2, 'Predição', [lista com letra de cada linha], True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
